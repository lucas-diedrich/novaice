{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from novaice.tl import ChemPertVAEModel\n",
    "import anndata as ad\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m Generating sequential column names                                                                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucas-diedrich/mamba/envs/hackathon/lib/python3.12/site-packages/scvi/train/_trainrunner.py:84: UserWarning: `accelerator` has been automatically set to `cpu` although 'mps' exists. If you wish to run on mps backend, use explicitly accelerator='mps' in train function.In future releases it will become default for mps supported machines.\n",
      "  accelerator, lightning_devices, device = parse_device_args(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/Users/lucas-diedrich/mamba/envs/hackathon/lib/python3.12/site-packages/lightning/pytorch/trainer/setup.py:166: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/lucas-diedrich/mamba/envs/hackathon/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=13` in the `DataLoader` to improve performance.\n",
      "/Users/lucas-diedrich/mamba/envs/hackathon/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0857b329adda4249a998830f7306dbc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    }
   ],
   "source": [
    "# Create example data\n",
    "n_samples = 100\n",
    "n_genes = 500\n",
    "embedding_dim = 768\n",
    "gene_expr = np.random.randn(n_samples, n_genes)  # Gene expression\n",
    "drug_emb = np.random.randn(n_samples, embedding_dim)  # Drug embeddings\n",
    "adata = ad.AnnData(X=gene_expr)\n",
    "adata.obsm[\"drug_embedding\"] = drug_emb\n",
    "# Setup and train model\n",
    "ChemPertVAEModel.setup_anndata(adata, drug_embedding_key=\"drug_embedding\")\n",
    "model = ChemPertVAEModel(adata)\n",
    "model.train(max_epochs=50)\n",
    "# Predict gene expression\n",
    "predictions = model.predict_gene_expression()\n",
    "# Get latent representation\n",
    "latent = model.get_latent_representation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.22947748,  0.15531465,  0.35715735, ...,  0.00895329,\n",
       "        -0.26845628,  0.05861602],\n",
       "       [-0.0047905 , -0.18331298, -0.06270131, ..., -0.06861049,\n",
       "        -0.04613385,  0.2090369 ],\n",
       "       [-0.2526181 , -0.01741746,  0.10985899, ...,  0.48693514,\n",
       "        -0.04255801,  0.44221842],\n",
       "       ...,\n",
       "       [-0.15090133,  0.22543375,  0.07259408, ..., -0.08813561,\n",
       "         0.06353926,  0.29402027],\n",
       "       [-0.40000558, -0.0069978 , -0.10733563, ...,  0.0219506 ,\n",
       "        -0.12074345,  0.1059868 ],\n",
       "       [-0.23956327, -0.07083929,  0.39903122, ..., -0.00906155,\n",
       "         0.32099187,  0.4492617 ]], shape=(100, 500), dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
