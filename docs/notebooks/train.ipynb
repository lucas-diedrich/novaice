{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example notebook (simulated data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/novaice/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from novaice.tl import ChemPertVAEModel, ChemPertMLPModel\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "from lightning.pytorch.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example data\n",
    "n_samples = 100\n",
    "n_genes = 500\n",
    "embedding_dim = 768\n",
    "gene_expr = np.random.randn(n_samples, n_genes)  # Gene expression\n",
    "drug_emb = np.random.randn(n_samples, embedding_dim)  # Drug embeddings\n",
    "adata = ad.AnnData(X=gene_expr)\n",
    "adata.obsm[\"drug_embedding\"] = drug_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m Generating sequential column names                                                                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/novaice/lib/python3.11/site-packages/scvi/train/_trainrunner.py:84: UserWarning: `accelerator` has been automatically set to `cpu` although 'mps' exists. If you wish to run on mps backend, use explicitly accelerator='mps' in train function.In future releases it will become default for mps supported machines.\n",
      "  accelerator, lightning_devices, device = parse_device_args(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/novaice/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:166: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/novaice/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=13` in the `DataLoader` to improve performance.\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/novaice/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=5). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/novaice/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:166: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/novaice/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=13` in the `DataLoader` to improve performance.\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/novaice/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=5). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 50/50 [00:00<00:00, 122.74it/s, v_num=2, train_loss=682]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 50/50 [00:00<00:00, 107.61it/s, v_num=2, train_loss=682]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup and train model\n",
    "ChemPertVAEModel.setup_anndata(adata, drug_embedding_key=\"drug_embedding\")\n",
    "model = ChemPertVAEModel(adata)\n",
    "\n",
    "# Create TensorBoard logger\n",
    "tb_logger = TensorBoardLogger(\"logs\", name=\"chempert_vae\")\n",
    "\n",
    "model.train(max_epochs=50,\n",
    "            logger=tb_logger,\n",
    "            log_every_n_steps=5, )\n",
    "# Predict gene expression\n",
    "predictions = model.predict_gene_expression()\n",
    "# Get latent representation\n",
    "latent = model.get_latent_representation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m Generating sequential column names                                                                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/novaice/lib/python3.11/site-packages/scvi/train/_trainrunner.py:84: UserWarning: `accelerator` has been automatically set to `cpu` although 'mps' exists. If you wish to run on mps backend, use explicitly accelerator='mps' in train function.In future releases it will become default for mps supported machines.\n",
      "  accelerator, lightning_devices, device = parse_device_args(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/novaice/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:166: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/novaice/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=13` in the `DataLoader` to improve performance.\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/novaice/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 50/50 [00:00<00:00, 138.65it/s, v_num=1, train_loss=468]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 50/50 [00:00<00:00, 137.17it/s, v_num=1, train_loss=468]\n"
     ]
    }
   ],
   "source": [
    "# Setup and train model\n",
    "ChemPertMLPModel.setup_anndata(adata, drug_embedding_key=\"drug_embedding\")\n",
    "model = ChemPertMLPModel(adata)\n",
    "model.train(max_epochs=50,\n",
    "            logger=tb_logger,\n",
    "            log_every_n_steps=5, )\n",
    "# Predict gene expression\n",
    "predictions = model.predict_gene_expression()\n",
    "# Get latent representation\n",
    "latent = model.get_prediction_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08490299,  0.01149954,  0.03652435,  0.1205297 ,  0.03714587,\n",
       "        0.06657201,  0.15247999,  0.10168385,  0.03969806, -0.01599456,\n",
       "       -0.00624166,  0.09742456,  0.10534384,  0.07294985,  0.11619731,\n",
       "        0.10446647,  0.09934682,  0.13611974,  0.03372545,  0.09709959,\n",
       "        0.05531964,  0.15708552,  0.06008263,  0.09644231,  0.04263443,\n",
       "        0.05792092,  0.06905067,  0.10266561,  0.15244341,  0.13504979,\n",
       "        0.07505822,  0.10024156, -0.03412777,  0.10176546, -0.04269175,\n",
       "        0.11077628,  0.09890586, -0.01874888,  0.0980655 ,  0.10427356,\n",
       "        0.06158306,  0.09025299,  0.06647262,  0.11493827, -0.00656793,\n",
       "       -0.00371751, -0.03355224,  0.05029252,  0.05604016,  0.11657432,\n",
       "        0.12093469,  0.12052019,  0.17429232,  0.10833763,  0.13482389,\n",
       "        0.11187175,  0.06493888,  0.07060604,  0.09511399,  0.04677574,\n",
       "        0.02701685,  0.10575106,  0.09443526,  0.04254399,  0.09967474,\n",
       "        0.08944699,  0.13641833,  0.03353988,  0.09925881,  0.05065863,\n",
       "        0.09819067,  0.07187741,  0.06011373,  0.0426449 ,  0.08580121,\n",
       "        0.07044313,  0.09811721,  0.09578827,  0.07284998,  0.11810157,\n",
       "        0.095905  ,  0.03420868,  0.03282408,  0.11009869,  0.05821476,\n",
       "        0.10576975,  0.05410589,  0.05867065,  0.09195441,  0.12182041,\n",
       "        0.11183851,  0.13992347,  0.15834513, -0.01964446,  0.08959699,\n",
       "        0.06991462,  0.07448807, -0.0382958 ,  0.07198447,  0.1044187 ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_prediction_error(method=\"r2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "novaice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
