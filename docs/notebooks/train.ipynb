{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example notebook (simulated data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from novaice.tl import ChemPertVAEModel, ChemPertMLPModel\n",
    "import anndata as ad\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example data\n",
    "n_samples = 100\n",
    "n_genes = 500\n",
    "embedding_dim = 768\n",
    "gene_expr = np.random.randn(n_samples, n_genes)  # Gene expression\n",
    "drug_emb = np.random.randn(n_samples, embedding_dim)  # Drug embeddings\n",
    "adata = ad.AnnData(X=gene_expr)\n",
    "adata.obsm[\"drug_embedding\"] = drug_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m Generating sequential column names                                                                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucas-diedrich/mamba/envs/hackathon/lib/python3.12/site-packages/scvi/train/_trainrunner.py:84: UserWarning: `accelerator` has been automatically set to `cpu` although 'mps' exists. If you wish to run on mps backend, use explicitly accelerator='mps' in train function.In future releases it will become default for mps supported machines.\n",
      "  accelerator, lightning_devices, device = parse_device_args(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/Users/lucas-diedrich/mamba/envs/hackathon/lib/python3.12/site-packages/lightning/pytorch/trainer/setup.py:166: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/lucas-diedrich/mamba/envs/hackathon/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=13` in the `DataLoader` to improve performance.\n",
      "/Users/lucas-diedrich/mamba/envs/hackathon/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa394d6bd9fb42229af60c37e53385c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    }
   ],
   "source": [
    "# Setup and train model\n",
    "ChemPertVAEModel.setup_anndata(adata, drug_embedding_key=\"drug_embedding\")\n",
    "model = ChemPertVAEModel(adata)\n",
    "model.train(max_epochs=50)\n",
    "# Predict gene expression\n",
    "predictions = model.predict_gene_expression()\n",
    "# Get latent representation\n",
    "latent = model.get_latent_representation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m Generating sequential column names                                                                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucas-diedrich/mamba/envs/hackathon/lib/python3.12/site-packages/scvi/train/_trainrunner.py:84: UserWarning: `accelerator` has been automatically set to `cpu` although 'mps' exists. If you wish to run on mps backend, use explicitly accelerator='mps' in train function.In future releases it will become default for mps supported machines.\n",
      "  accelerator, lightning_devices, device = parse_device_args(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/Users/lucas-diedrich/mamba/envs/hackathon/lib/python3.12/site-packages/lightning/pytorch/trainer/setup.py:166: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/lucas-diedrich/mamba/envs/hackathon/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=13` in the `DataLoader` to improve performance.\n",
      "/Users/lucas-diedrich/mamba/envs/hackathon/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8034e22a1509432283fdc4ee966118ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    }
   ],
   "source": [
    "# Setup and train model\n",
    "ChemPertMLPModel.setup_anndata(adata, drug_embedding_key=\"drug_embedding\")\n",
    "model = ChemPertMLPModel(adata)\n",
    "model.train(max_epochs=50)\n",
    "# Predict gene expression\n",
    "predictions = model.predict_gene_expression()\n",
    "# Get latent representation\n",
    "latent = model.get_prediction_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08062749,  0.09162999,  0.0323052 ,  0.10616307,  0.09551314,\n",
       "        0.10559062,  0.03638097, -0.04228011,  0.11437934,  0.11619559,\n",
       "        0.13928162,  0.0689207 ,  0.06799291, -0.00320087,  0.07779458,\n",
       "       -0.04287967,  0.077312  , -0.0151089 ,  0.12280114,  0.03974983,\n",
       "        0.09240473,  0.0833765 ,  0.06498342,  0.04111685,  0.10994626,\n",
       "        0.14451183,  0.07440296,  0.13742468,  0.08721003,  0.10177605,\n",
       "        0.05887708,  0.1774986 ,  0.12156647,  0.02645941,  0.16160898,\n",
       "        0.07046322,  0.18522587, -0.06495865,  0.06359713, -0.01569936,\n",
       "        0.00361843,  0.0793735 ,  0.23512301,  0.05725976,  0.05842183,\n",
       "        0.10827314,  0.1226686 ,  0.10583632,  0.06769605,  0.06339478,\n",
       "        0.09591005,  0.06394573,  0.18466068,  0.05429771,  0.0886385 ,\n",
       "        0.19613076,  0.10455579, -0.03458726,  0.07521188,  0.04424723,\n",
       "        0.09827788,  0.03463786,  0.12628488,  0.13443344,  0.07028681,\n",
       "        0.11729928,  0.14073009, -0.03576906,  0.04965198,  0.04903353,\n",
       "        0.03146627,  0.07788058, -0.02290962,  0.08858492,  0.1211715 ,\n",
       "        0.0654579 ,  0.06875261,  0.09017576,  0.05858322,  0.04235645,\n",
       "        0.07587874,  0.04075659,  0.06895371,  0.16767263,  0.0493835 ,\n",
       "        0.04298225,  0.05495388,  0.05380878,  0.1230657 ,  0.06545827,\n",
       "        0.05489387,  0.11286651,  0.199884  ,  0.09045144,  0.21374341,\n",
       "        0.11008054,  0.06293788,  0.06935312,  0.03411147,  0.13705392])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_prediction_error(method=\"r2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
